{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b163871",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86cf747d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tian\\AppData\\Local\\Temp\\ipykernel_13020\\614879992.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset shape: (1048565, 4)\n",
      "    Sentence #           Word  POS    Tag\n",
      "0  Sentence: 1      Thousands  NNS      O\n",
      "1  Sentence: 1             of   IN      O\n",
      "2  Sentence: 1  demonstrators  NNS      O\n",
      "3  Sentence: 1           have  VBP      O\n",
      "4  Sentence: 1        marched  VBN      O\n",
      "5  Sentence: 1        through   IN      O\n",
      "6  Sentence: 1         London  NNP  B-geo\n",
      "7  Sentence: 1             to   TO      O\n",
      "8  Sentence: 1        protest   VB      O\n",
      "9  Sentence: 1            the   DT      O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (adjust encoding if needed)\n",
    "df = pd.read_csv(r\"C:\\Users\\Tian\\Desktop\\NLP_Task7\\Datasets\\NER dataset.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "# Step 1: Fill missing 'Sentence #' by forward filling\n",
    "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n",
    "\n",
    "# Step 2: Drop any rows with missing values in required columns\n",
    "df = df.dropna(subset=['Word', 'POS', 'Tag'])\n",
    "\n",
    "# Step 3: Strip whitespace from strings in all relevant columns\n",
    "df['Word'] = df['Word'].str.strip()\n",
    "df['POS'] = df['POS'].str.strip()\n",
    "df['Tag'] = df['Tag'].str.strip()\n",
    "\n",
    "# Step 4: Preview the cleaned dataset\n",
    "print(\"✅ Cleaned dataset shape:\", df.shape)\n",
    "print(df.head(10))\n",
    "\n",
    "# Optional: Save cleaned data to new CSV\n",
    "df.to_csv(r\"C:\\Users\\Tian\\Desktop\\NLP_Task7\\NER_Recog\\cleaned_NER_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f362ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV (already cleaned based on your previous code)\n",
    "df = pd.read_csv(r\"C:\\Users\\Tian\\Desktop\\NLP_Task7\\NER_Recog\\cleaned_NER_dataset.csv\")\n",
    "\n",
    "# Group by sentence\n",
    "grouped = df.groupby(\"Sentence #\").agg(list)\n",
    "\n",
    "# Extract sequences\n",
    "sentences = grouped['Word'].tolist()\n",
    "ner_tags = grouped['Tag'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24fb766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91df39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all elements in sentences are valid strings\n",
    "cleaned_sentences = [[str(word) if not isinstance(word, str) else word for word in sentence] for sentence in sentences]\n",
    "\n",
    "X = [extract_features(s) for s in cleaned_sentences]  # List of list of dicts\n",
    "y = ner_tags  # List of list of tags (already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c78078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn_crfsuite\n",
    "\n",
    "# crf = sklearn_crfsuite.CRF(\n",
    "#     algorithm='lbfgs',\n",
    "#     c1=0.1,\n",
    "#     c2=0.1,\n",
    "#     max_iterations=100,\n",
    "#     all_possible_transitions=True\n",
    "# )\n",
    "\n",
    "# crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07c7c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn_crfsuite import metrics\n",
    "\n",
    "# y_pred = crf.predict(X)\n",
    "\n",
    "# print(metrics.flat_classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d14a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build word and tag vocabularies\n",
    "all_words = [word for sentence in sentences for word in sentence]\n",
    "all_tags = [tag for tag_list in ner_tags for tag in tag_list]\n",
    "\n",
    "word_vocab = {word: i + 2 for i, (word, _) in enumerate(Counter(all_words).items())}\n",
    "tag_vocab = {tag: i for i, tag in enumerate(set(all_tags))}\n",
    "tag_vocab['<PAD>'] = len(tag_vocab)\n",
    "\n",
    "# Add special tokens\n",
    "word_vocab['<PAD>'] = 0\n",
    "word_vocab['<UNK>'] = 1\n",
    "\n",
    "# Reverse vocab for later use\n",
    "reverse_word_vocab = {v: k for k, v in word_vocab.items()}\n",
    "reverse_tag_vocab = {v: k for k, v in tag_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fece513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word_vocab):\n",
    "    return [word_vocab.get(word, word_vocab['<UNK>']) for word in sentence]\n",
    "\n",
    "def tags_to_indices(tags, tag_vocab):\n",
    "    return [tag_vocab[tag] for tag in tags]\n",
    "\n",
    "X_data = [sentence_to_indices(sentence, word_vocab) for sentence in sentences]\n",
    "y_data = [tags_to_indices(tags, tag_vocab) for tags in ner_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b69f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "\n",
    "X_data_padded = pad_sequence([torch.tensor(sentence) for sentence in X_data], batch_first=True, padding_value=word_vocab['<PAD>'])\n",
    "y_data_padded = pad_sequence([torch.tensor(tags) for tags in y_data], batch_first=True, padding_value=tag_vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5cd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNN_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
    "        super(RNN_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_vocab['<PAD>'])\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        # ✅ Pack padded sequence (lengths must be on CPU!)\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=x.size(1))  # ✅ Correct unpack\n",
    "        logits = self.fc(output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7f43c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_NER(vocab_size=len(word_vocab), tagset_size=len(tag_vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tag_vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd5defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 9992.3239, Accuracy: 0.9410, Time: 787.60s\n",
      "Epoch 2/5, Loss: 6137.5454, Accuracy: 0.9611, Time: 778.81s\n",
      "Epoch 3/5, Loss: 5360.0123, Accuracy: 0.9654, Time: 782.75s\n",
      "Epoch 4/5, Loss: 4949.4424, Accuracy: 0.9680, Time: 770.17s\n",
      "Epoch 5/5, Loss: 4725.9290, Accuracy: 0.9693, Time: 755.59s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 5\n",
    "model.train()\n",
    "\n",
    "# Make sure lengths are calculated from padded input\n",
    "lengths = [torch.count_nonzero(seq != word_vocab['<PAD>']).item() for seq in X_data_padded]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(len(X_data_padded)):\n",
    "        inputs = X_data_padded[i].unsqueeze(0)  # (1, seq_len)\n",
    "        targets = y_data_padded[i].unsqueeze(0)  # (1, seq_len)\n",
    "        seq_len = torch.tensor([lengths[i]])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, seq_len)  # (1, seq_len, num_tags)\n",
    "\n",
    "        outputs_flat = outputs.view(-1, len(tag_vocab))\n",
    "        targets_flat = targets.view(-1)\n",
    "\n",
    "        loss = loss_fn(outputs_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        predictions = torch.argmax(outputs, dim=-1)  # (1, seq_len)\n",
    "        mask = targets != tag_vocab['<PAD>']\n",
    "        correct = (predictions == targets) & mask\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    epoch_accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Time: {elapsed_time:.2f}s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6277089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art     0.2132    0.1045    0.1402       402\n",
      "       B-eve     0.7043    0.2630    0.3830       308\n",
      "       B-geo     0.8398    0.9281    0.8818     37644\n",
      "       B-gpe     0.9734    0.9270    0.9497     15870\n",
      "       B-nat     0.7079    0.3134    0.4345       201\n",
      "       B-org     0.7874    0.7232    0.7540     20143\n",
      "       B-per     0.8483    0.8435    0.8459     16990\n",
      "       B-tim     0.9374    0.8649    0.8997     20333\n",
      "       I-art     0.1031    0.0774    0.0885       297\n",
      "       I-eve     0.5909    0.1028    0.1751       253\n",
      "       I-geo     0.7704    0.8183    0.7936      7414\n",
      "       I-gpe     0.9623    0.5152    0.6711       198\n",
      "       I-nat     0.8000    0.3137    0.4507        51\n",
      "       I-org     0.8293    0.7914    0.8099     16784\n",
      "       I-per     0.9182    0.8329    0.8735     17251\n",
      "       I-tim     0.7830    0.8194    0.8008      6528\n",
      "           O     0.9910    0.9939    0.9925    887898\n",
      "\n",
      "    accuracy                         0.9709   1048565\n",
      "   macro avg     0.7506    0.6019    0.6438   1048565\n",
      "weighted avg     0.9706    0.9709    0.9704   1048565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_data_padded)):\n",
    "        inputs = X_data_padded[i].unsqueeze(0)  # (1, seq_len)\n",
    "        targets = y_data_padded[i].unsqueeze(0)  # (1, seq_len)\n",
    "        seq_len = torch.tensor([lengths[i]])\n",
    "\n",
    "        outputs = model(inputs, seq_len)\n",
    "        predictions = torch.argmax(outputs, dim=-1)  # (1, seq_len)\n",
    "\n",
    "        pred = predictions.squeeze(0).tolist()\n",
    "        label = targets.squeeze(0).tolist()\n",
    "\n",
    "        for p, l in zip(pred, label):\n",
    "            if l != tag_vocab['<PAD>']:\n",
    "                all_preds.append(p)\n",
    "                all_labels.append(l)\n",
    "\n",
    "# Convert indices back to tag names\n",
    "all_preds_tags = [reverse_tag_vocab[i] for i in all_preds]\n",
    "all_labels_tags = [reverse_tag_vocab[i] for i in all_labels]\n",
    "\n",
    "# Evaluation report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels_tags, all_preds_tags, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
